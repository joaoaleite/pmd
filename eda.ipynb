{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as mcolors\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "from scipy.stats import norm\n",
    "from scipy.spatial.distance import jensenshannon\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from scipy.stats import pointbiserialr\n",
    "from scipy.stats import fisher_exact"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rebuild_articles(df):\n",
    "    combined_text = df.groupby(\"id_article\")[\"text\"].apply(\" \".join).reset_index()\n",
    "\n",
    "    combined_techniques = (\n",
    "        df.groupby(\"id_article\")[\"persuasion_techniques\"]\n",
    "        .apply(\n",
    "            lambda x: list(\n",
    "                set(technique for techniques in x.dropna() for technique in techniques)\n",
    "            )\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    combined_entities = (\n",
    "        df.groupby(\"id_article\")[\"entities\"]\n",
    "        .apply(\n",
    "            lambda x: json.dumps(\n",
    "                [\n",
    "                    dict(t)\n",
    "                    for t in {\n",
    "                        tuple(d.items()) for entities in x.dropna() for d in entities\n",
    "                    }\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        .reset_index()\n",
    "    )\n",
    "\n",
    "    labels = df.groupby(\"id_article\")[\"label\"].first().reset_index()\n",
    "\n",
    "    result = (\n",
    "        combined_text.merge(combined_techniques, on=\"id_article\")\n",
    "        .merge(combined_entities, on=\"id_article\")\n",
    "        .merge(labels, on=\"id_article\")\n",
    "    )\n",
    "\n",
    "    result[\"entities\"] = result[\"entities\"].apply(json.loads)\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEMEVAL_LABELS = [\n",
    "    \"Appeal_to_Authority\",\n",
    "    \"Appeal_to_Fear-Prejudice\",\n",
    "    \"Appeal_to_Hypocrisy\",\n",
    "    # \"Appeal_to_Popularity\",\n",
    "    # \"Appeal_to_Time\",\n",
    "    \"Appeal_to_Values\",\n",
    "    \"Causal_Oversimplification\",\n",
    "    # \"Consequential_Oversimplification\",\n",
    "    \"Conversation_Killer\",\n",
    "    \"Doubt\",\n",
    "    \"Exaggeration-Minimisation\",\n",
    "    \"False_Dilemma-No_Choice\",\n",
    "    \"Flag_Waving\",\n",
    "    \"Guilt_by_Association\",\n",
    "    \"Loaded_Language\",\n",
    "    \"Name_Calling-Labeling\",\n",
    "    # \"Obfuscation-Vagueness-Confusion\",\n",
    "    \"Questioning_the_Reputation\",\n",
    "    # \"Red_Herring\",\n",
    "    \"Repetition\",\n",
    "    \"Slogans\",\n",
    "    # \"Straw_Man\",\n",
    "    # \"Whataboutism\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(dataset_name):\n",
    "    if dataset_name == \"cidii\":\n",
    "        cidii_df = pd.read_csv(\"datasets/processed/cidii.csv\")\n",
    "        cidii_df[\"entities\"] = cidii_df[\"entities\"].apply(json.loads)\n",
    "        cidii_df[\"persuasion_techniques\"] = cidii_df[\"persuasion_techniques\"].apply(\n",
    "            lambda x: x.split(\",\") if isinstance(x, str) else []\n",
    "        )\n",
    "        cidii_df[\"persuasion_techniques\"] = cidii_df[\"persuasion_techniques\"].apply(\n",
    "            lambda x: [t for t in x if t in SEMEVAL_LABELS]\n",
    "        )\n",
    "        cidii_df = cidii_df[cidii_df[\"label\"] == 1].reset_index(drop=True)\n",
    "\n",
    "        return cidii_df\n",
    "\n",
    "    elif dataset_name == \"covid\":\n",
    "        covid_df = pd.read_csv(\"datasets/processed/covid.csv\")\n",
    "        covid_df[\"entities\"] = covid_df[\"entities\"].apply(json.loads)\n",
    "        covid_df[\"persuasion_techniques\"] = covid_df[\"persuasion_techniques\"].apply(\n",
    "            lambda x: x.split(\",\") if isinstance(x, str) else []\n",
    "        )\n",
    "        covid_df[\"persuasion_techniques\"] = covid_df[\"persuasion_techniques\"].apply(\n",
    "            lambda x: [t for t in x if t in SEMEVAL_LABELS]\n",
    "        )\n",
    "        covid_df = covid_df[covid_df[\"label\"] == 1].reset_index(drop=True)\n",
    "\n",
    "        return covid_df\n",
    "\n",
    "    elif dataset_name == \"climate_fever\":\n",
    "        climate_fever_df = pd.read_csv(\"datasets/processed/climate_fever.csv\")\n",
    "        climate_fever_df[\"entities\"] = climate_fever_df[\"entities\"].apply(json.loads)\n",
    "        climate_fever_df[\"persuasion_techniques\"] = climate_fever_df[\n",
    "            \"persuasion_techniques\"\n",
    "        ].apply(lambda x: x.split(\",\") if isinstance(x, str) else [])\n",
    "        climate_fever_df[\"persuasion_techniques\"] = climate_fever_df[\n",
    "            \"persuasion_techniques\"\n",
    "        ].apply(lambda x: [t for t in x if t in SEMEVAL_LABELS])\n",
    "        climate_fever_df = climate_fever_df[climate_fever_df[\"label\"] == 1].reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "\n",
    "        return climate_fever_df\n",
    "\n",
    "    elif dataset_name == \"euvsdisinfo\":\n",
    "        euvsdisinfo_df = pd.read_csv(\"datasets/processed/euvsdisinfo.csv\")\n",
    "        euvsdisinfo_df[\"entities\"] = euvsdisinfo_df[\"entities\"].apply(json.loads)\n",
    "        euvsdisinfo_df[\"persuasion_techniques\"] = euvsdisinfo_df[\n",
    "            \"persuasion_techniques\"\n",
    "        ].apply(lambda x: x.split(\",\") if isinstance(x, str) else [])\n",
    "        euvsdisinfo_df[\"persuasion_techniques\"] = euvsdisinfo_df[\n",
    "            \"persuasion_techniques\"\n",
    "        ].apply(lambda x: [t for t in x if t in SEMEVAL_LABELS])\n",
    "        euvsdisinfo_df = euvsdisinfo_df[euvsdisinfo_df[\"label\"] == 1].reset_index(\n",
    "            drop=True\n",
    "        )\n",
    "        euvsdisinfo_df = euvsdisinfo_df[\n",
    "            euvsdisinfo_df[\"keywords\"]\n",
    "            .apply(lambda x: x.split(\",\") if isinstance(x, str) else [])\n",
    "            .apply(lambda x: \"War in Ukraine\" in x)\n",
    "        ].reset_index(drop=True)\n",
    "\n",
    "        return euvsdisinfo_df\n",
    "\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown dataset name: {dataset_name}\")\n",
    "\n",
    "\n",
    "cidii_df = load_dataset(\"cidii\")\n",
    "covid_df = load_dataset(\"covid\")\n",
    "climate_fever_df = load_dataset(\"climate_fever\")\n",
    "euvsdisinfo_df = load_dataset(\"euvsdisinfo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n_entities_per_persuasion_technique(df, technique, n=None):\n",
    "    result = {}\n",
    "    entities = df[df[\"persuasion_techniques\"].apply(lambda x: technique in x)][\n",
    "        \"entities\"\n",
    "    ]\n",
    "\n",
    "    entity_counter = Counter(\n",
    "        (entity[\"text\"].lower(), entity[\"label\"])\n",
    "        for entities in entities\n",
    "        for entity in entities\n",
    "        if entity[\"label\"]\n",
    "    )\n",
    "\n",
    "    return [\n",
    "        (entity[0], entity[1], count) for entity, count in entity_counter.most_common(n)\n",
    "    ]\n",
    "\n",
    "\n",
    "def generate_wordcloud_data(entities_data):\n",
    "    \"\"\"\n",
    "    Prepares data for the word cloud, organizing entities by type and frequency.\n",
    "    \"\"\"\n",
    "    word_freq_by_type = defaultdict(dict)\n",
    "    for item in entities_data:\n",
    "        if isinstance(item, tuple) and len(item) == 3:\n",
    "            entity, label, count = item\n",
    "            word_freq_by_type[label][entity] = count\n",
    "        else:\n",
    "            print(f\"Skipping item due to incorrect format: {item}\")\n",
    "    return word_freq_by_type\n",
    "\n",
    "\n",
    "def assign_colors_to_labels(labels):\n",
    "    \"\"\"\n",
    "    Assigns unique colors to each label across all subplots.\n",
    "    \"\"\"\n",
    "    color_palette = [\n",
    "        \"#E63946\",  # Bright Red\n",
    "        \"#457B9D\",  # Teal Blue\n",
    "        \"#2A9D8F\",  # Jade Green\n",
    "        \"#8B4513\",  # Brown\n",
    "        \"#F77F00\",  # Vivid Orange\n",
    "        \"#8D99AE\",  # Slate Gray\n",
    "        \"#264653\",  # Deep Blue\n",
    "        \"#FFDD00\",  # Bright Yellow\n",
    "        \"#06D6A0\",  # Aqua Green\n",
    "        \"#3D348B\",  # Deep Purple\n",
    "        \"#FF1493\",  # Bright Pink\n",
    "        \"#73A942\",  # Olive Green\n",
    "        \"#006400\",  # Deep Green\n",
    "        \"#DAA520\",  # Golden\n",
    "        \"#2803fc\",  # Deep blue\n",
    "        \"#FF00FF\",  # Magenta\n",
    "        \"#00FFFF\",  # Cyan\n",
    "        \"#FF4500\",  # OrangeRed\n",
    "    ]\n",
    "    label_colors = {\n",
    "        label: color_palette[i % len(color_palette)] for i, label in enumerate(labels)\n",
    "    }\n",
    "    return label_colors\n",
    "\n",
    "\n",
    "def color_func(word, **kwargs):\n",
    "    \"\"\"\n",
    "    Color function that retrieves the color based on the word's entity label.\n",
    "    \"\"\"\n",
    "    label = word_to_label.get(word, None)\n",
    "    return label_colors.get(label, \"#000000\")\n",
    "\n",
    "\n",
    "def plot_wordclouds_with_legend(*datasets):\n",
    "    \"\"\"\n",
    "    Plots a word cloud for each dataset, with words colored according to entity type.\n",
    "    \"\"\"\n",
    "    fig, axes = plt.subplots(4, 1, figsize=(6, 12))\n",
    "    global label_colors, word_to_label\n",
    "    word_to_label = {}\n",
    "\n",
    "    present_labels = set()\n",
    "    for data, _ in datasets:\n",
    "        word_freq_by_type = generate_wordcloud_data(data)\n",
    "        present_labels.update(word_freq_by_type.keys())\n",
    "\n",
    "    label_colors = assign_colors_to_labels(present_labels)\n",
    "\n",
    "    legend_labels = set()\n",
    "\n",
    "    for i, (data, title) in enumerate(datasets):\n",
    "        word_freq_by_type = generate_wordcloud_data(data)\n",
    "        wc = WordCloud(width=1000, height=300, background_color=\"white\")\n",
    "\n",
    "        combined_frequencies = {}\n",
    "        for label, words in word_freq_by_type.items():\n",
    "            for word, count in words.items():\n",
    "                combined_frequencies[word] = count\n",
    "                word_to_label[word] = label\n",
    "                legend_labels.add(label)\n",
    "\n",
    "        wc.generate_from_frequencies(combined_frequencies)\n",
    "\n",
    "        axes[i].imshow(wc.recolor(color_func=color_func), interpolation=\"bilinear\")\n",
    "        axes[i].set_title(title, fontsize=14, weight=\"bold\")\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    fig.subplots_adjust(hspace=0.4)\n",
    "\n",
    "    filtered_handles = [\n",
    "        plt.Line2D([0], [0], color=label_colors[label], lw=4) for label in legend_labels\n",
    "    ]\n",
    "    filtered_labels = [label for label in legend_labels]\n",
    "    fig.legend(\n",
    "        filtered_handles,\n",
    "        filtered_labels,\n",
    "        title=\"Entity Type\",\n",
    "        fontsize=8,\n",
    "        title_fontsize=10,\n",
    "        loc=\"lower center\",\n",
    "        ncol=5,\n",
    "        bbox_to_anchor=(0.5, 0.17),\n",
    "    )\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(bottom=0.25)\n",
    "    plt.savefig(\"figures/wordclouds.pdf\", bbox_inches=\"tight\", dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "technique = \"Appeal_to_Authority\"\n",
    "cidii_df_entities = get_top_n_entities_per_persuasion_technique(\n",
    "    cidii_df, n=50, technique=technique\n",
    ")\n",
    "covid_df_entities = get_top_n_entities_per_persuasion_technique(\n",
    "    covid_df, n=50, technique=technique\n",
    ")\n",
    "climate_fever_df_entities = get_top_n_entities_per_persuasion_technique(\n",
    "    climate_fever_df, n=50, technique=technique\n",
    ")\n",
    "euvsdisinfo_df_entities = get_top_n_entities_per_persuasion_technique(\n",
    "    euvsdisinfo_df, n=50, technique=technique\n",
    ")\n",
    "\n",
    "# Plot word clouds with legend\n",
    "plot_wordclouds_with_legend(\n",
    "    (cidii_df_entities, \"CIDII\"),\n",
    "    (covid_df_entities, \"COVID\"),\n",
    "    (climate_fever_df_entities, \"Climate Fever\"),\n",
    "    (euvsdisinfo_df_entities, \"EUvsDisinfo\"),\n",
    ")\n",
    "\n",
    "\n",
    "#  Credit to https://towardsdatascience.com/explorations-in-named-entity-recognition-and-was-eleanor-roosevelt-right-671271117218\n",
    "# PERSON:      People, including fictional.\n",
    "# NORP:        Nationalities or religious or political groups.\n",
    "# FAC:         Buildings, airports, highways, bridges, etc.\n",
    "# ORG:         Companies, agencies, institutions, etc.\n",
    "# GPE:         Countries, cities, states.\n",
    "# LOC:         Non-GPE locations, mountain ranges, bodies of water.\n",
    "# PRODUCT:     Objects, vehicles, foods, etc. (Not services.)\n",
    "# EVENT:       Named hurricanes, battles, wars, sports events, etc.\n",
    "# WORK_OF_ART: Titles of books, songs, etc.\n",
    "# LAW:         Named documents made into laws.\n",
    "# LANGUAGE:    Any named language.\n",
    "# DATE:        Absolute or relative dates or periods.\n",
    "# TIME:        Times smaller than a day.\n",
    "# PERCENT:     Percentage, including ”%“.\n",
    "# MONEY:       Monetary values, including unit.\n",
    "# QUANTITY:    Measurements, as of weight or distance.\n",
    "# ORDINAL:     “first”, “second”, etc.\n",
    "# CARDINAL:    Numerals that do not fall under another type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of appeal to authority mentioning an entity\n",
    "climate_fever_df[climate_fever_df[\"text\"].str.casefold().str.contains(\"ipcc\")].iloc[4][\n",
    "    \"text\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of exaggeration-minimisation\n",
    "climate_fever_df[\n",
    "    climate_fever_df[\"persuasion_techniques\"].apply(\n",
    "        lambda x: \"Exaggeration-Minimisation\" in x\n",
    "    )\n",
    "].iloc[4][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of questioning the reputation with a text length smaller than 100\n",
    "df = euvsdisinfo_df[\n",
    "    euvsdisinfo_df[\"persuasion_techniques\"].apply(\n",
    "        lambda x: \"Questioning_the_Reputation\" in x\n",
    "    )\n",
    "]\n",
    "df[df[\"text\"].str.len() < 100].iloc[9][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of appeal to fear in covid\n",
    "df = covid_df[\n",
    "    covid_df[\"persuasion_techniques\"].apply(lambda x: \"Appeal_to_Fear-Prejudice\" in x)\n",
    "]\n",
    "df[df[\"text\"].str.len() < 100].iloc[42][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of repetition in cidii\n",
    "df = cidii_df[cidii_df[\"persuasion_techniques\"].apply(lambda x: \"Repetition\" in x)]\n",
    "print(df[df[\"text\"].str.len() > 100].iloc[2][\"text\"])\n",
    "print(df[df[\"text\"].str.len() > 100].iloc[8][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# example of repetition in cidii\n",
    "df = cidii_df[\n",
    "    cidii_df[\"persuasion_techniques\"].apply(lambda x: \"Name_Calling-Labeling\" in x)\n",
    "]\n",
    "print(df[df[\"text\"].str.len() > 100].iloc[2][\"text\"])\n",
    "print(df[df[\"text\"].str.len() > 100].iloc[8][\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cidii_df[\n",
    "    (cidii_df[\"persuasion_techniques\"].apply(lambda x: \"Appeal_to_Fear-Prejudice\" in x))\n",
    "    & (cidii_df[\"label\"] == 1)\n",
    "    # & (cidii_df[\"text\"].apply(lambda x: \"?\" in x))\n",
    "][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "covid_df[\n",
    "    (covid_df[\"persuasion_techniques\"].apply(lambda x: \"Appeal_to_Fear-Prejudice\" in x))\n",
    "    & (covid_df[\"label\"] == 1)\n",
    "    # & (covid_df[\"text\"].apply(lambda x: \"?\" in x))\n",
    "][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "climate_fever_df[\n",
    "    (\n",
    "        climate_fever_df[\"persuasion_techniques\"].apply(\n",
    "            lambda x: \"Appeal_to_Fear-Prejudice\" in x\n",
    "        )\n",
    "    )\n",
    "    & (climate_fever_df[\"label\"] == 1)\n",
    "][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "euvsdisinfo_df[\n",
    "    (\n",
    "        euvsdisinfo_df[\"persuasion_techniques\"].apply(\n",
    "            lambda x: \"Appeal_to_Fear-Prejudice\" in x\n",
    "        )\n",
    "    )\n",
    "    & (climate_fever_df[\"label\"] == 1)\n",
    "][\"text\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_df_statistics(df):\n",
    "    sent_str_len = df[\"text\"].str.len().mean()\n",
    "    num_sent = len(df)\n",
    "    avg_persuasion_per_text = df[\"persuasion_techniques\"].apply(lambda x: len(x)).mean()\n",
    "    avg_entities_per_text = df[\"entities\"].apply(len).mean()\n",
    "\n",
    "    df = rebuild_articles(df)\n",
    "    num_articles = len(df)\n",
    "    # num_true = df[\"label\"].sum()\n",
    "    # num_false = len(df) - num_true\n",
    "\n",
    "    return {\n",
    "        \"# Articles\": num_articles,\n",
    "        # \"# True\": num_true,\n",
    "        # \"# False\": num_false,\n",
    "        \"# Sentences\": num_sent,\n",
    "        \"Avg. Sentence Length\": np.round(sent_str_len, 1),\n",
    "        \"Avg. Persuasion Techniques per Sentence\": np.round(avg_persuasion_per_text, 1),\n",
    "        \"Avg. Entities per Sentence\": np.round(avg_entities_per_text, 1),\n",
    "    }\n",
    "\n",
    "\n",
    "rows = []\n",
    "for dataset in [cidii_df, covid_df, climate_fever_df, euvsdisinfo_df]:\n",
    "    rows.append(get_df_statistics(dataset))\n",
    "\n",
    "statistics_df = pd.DataFrame(\n",
    "    rows, index=[\"CIDII\", \"COVID\", \"Climate Fever\", \"EUvsDisinfo\"]\n",
    ").T\n",
    "\n",
    "statistics_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pt_distribution_single_bar(*dfs, labels, semeval_labels=None):\n",
    "    def prepare_data(df):\n",
    "        if \"persuasion_techniques\" not in df.columns:\n",
    "            raise ValueError(\"DataFrame must contain 'persuasion_techniques' column.\")\n",
    "\n",
    "        df[\"techniques_list\"] = df[\"persuasion_techniques\"]\n",
    "        df_exploded = df.explode(\"techniques_list\")\n",
    "        df_exploded = df_exploded.dropna(subset=[\"techniques_list\"]) \n",
    "\n",
    "        technique_counts = Counter(df_exploded[\"techniques_list\"])\n",
    "        df_counts = (\n",
    "            pd.DataFrame.from_dict(technique_counts, orient=\"index\", columns=[\"Count\"])\n",
    "            .reindex(semeval_labels)\n",
    "            .fillna(0)\n",
    "        )\n",
    "        total_count = df_counts[\"Count\"].sum()\n",
    "\n",
    "        if total_count > 0:\n",
    "            df_counts[\"Relative_Frequency (%)\"] = (\n",
    "                df_counts[\"Count\"] / total_count\n",
    "            ) * 100\n",
    "        else:\n",
    "            df_counts[\"Relative_Frequency (%)\"] = 0\n",
    "\n",
    "        return df_counts[[\"Relative_Frequency (%)\"]].sort_index(), total_count\n",
    "\n",
    "    prepared_data = []\n",
    "    total_counts = {}\n",
    "    for df, label in zip(dfs, labels):\n",
    "        df_counts, total_count = prepare_data(df)\n",
    "        prepared_data.append(df_counts[\"Relative_Frequency (%)\"])\n",
    "        total_counts[label] = total_count\n",
    "\n",
    "    combined_df = pd.DataFrame(prepared_data, index=labels).T\n",
    "\n",
    "    combined_df[\"Total_Frequency\"] = combined_df.sum(axis=1)\n",
    "    combined_df = combined_df.sort_values(by=\"Total_Frequency\", ascending=True).drop(\n",
    "        columns=\"Total_Frequency\"\n",
    "    )\n",
    "\n",
    "    max_frequency = combined_df.values.max()\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 5))\n",
    "    bar_height = 0.20\n",
    "    indices = np.arange(len(combined_df))\n",
    "\n",
    "    for i, label in enumerate(labels):\n",
    "        ax.barh(indices + i * bar_height, combined_df[label], bar_height, label=label)\n",
    "\n",
    "    ax.set_xlabel(\"Proportion (%)\", fontsize=10)\n",
    "    ax.set_yticks(indices + bar_height * (len(labels) - 1) / 2)\n",
    "\n",
    "    ax.set_yticklabels([idx.replace(\"_\", \" \") for idx in combined_df.index], fontsize=10)\n",
    "    ax.set_xlim(0, max_frequency + 5)\n",
    "    ax.legend(fontsize=8, title=\"Disinformation Domain\", title_fontsize=10)\n",
    "\n",
    "    plt.grid(axis=\"x\", linestyle=\"--\", alpha=0.6)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"figures/pt_distribution.pdf\", bbox_inches=\"tight\", dpi=600)\n",
    "    plt.show()\n",
    "\n",
    "    return combined_df, [v for k, v in total_counts.items()], list(reversed(combined_df.index))\n",
    "\n",
    "\n",
    "# cidii_df_rebuilt = rebuild_articles(cidii_df)\n",
    "# covid_df_rebuilt = rebuild_articles(covid_df)\n",
    "# climate_fever_df_rebuilt = rebuild_articles(climate_fever_df)\n",
    "# euvsdisinfo_df_rebuilt = rebuild_articles(euvsdisinfo_df)\n",
    "\n",
    "# cidii_df[\"persuasion_techniques\"] = cidii_df[\n",
    "#     \"persuasion_techniques\"\n",
    "# ].apply(lambda x: \",\".join(x))\n",
    "# covid_df[\"persuasion_techniques\"] = covid_df[\n",
    "#     \"persuasion_techniques\"\n",
    "# ].apply(lambda x: \",\".join(x))\n",
    "# climate_fever_df[\"persuasion_techniques\"] = climate_fever_df[\n",
    "#     \"persuasion_techniques\"\n",
    "# ].apply(lambda x: \",\".join(x))\n",
    "# euvsdisinfo_df[\"persuasion_techniques\"] = euvsdisinfo_df[\n",
    "#     \"persuasion_techniques\"\n",
    "# ].apply(lambda x: \",\".join(x))\n",
    "\n",
    "frequencies_df, total_counts, yticklabels = plot_pt_distribution_single_bar(\n",
    "    cidii_df,\n",
    "    covid_df,\n",
    "    climate_fever_df,\n",
    "    euvsdisinfo_df,\n",
    "    labels=(\"Islamic Issues\", \"COVID-19\", \"Climate Change\", \"Russo-Ukrainian War\"),\n",
    "    semeval_labels=SEMEVAL_LABELS,\n",
    ")\n",
    "\n",
    "\n",
    "def highlight_max_in_row(row):\n",
    "    is_max = row == row.max()\n",
    "    return [\"background-color: red\" if v else \"\" for v in is_max]\n",
    "\n",
    "\n",
    "styled_df = frequencies_df.style.apply(highlight_max_in_row, axis=1)\n",
    "styled_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = [\"CIDII\", \"COVID\", \"Climate Fever\", \"EUvsDisinfo\"]\n",
    "frequencies_df = frequencies_df.loc[SEMEVAL_LABELS]\n",
    "data_proportions = frequencies_df.T.to_numpy() / 100\n",
    "# Compute pairwise Jensen-Shannon divergence between each dataset\n",
    "js_divergence_results = {}\n",
    "for i in range(len(data_proportions)):\n",
    "    for j in range(i + 1, len(data_proportions)):\n",
    "        # Calculate Jensen-Shannon divergence\n",
    "        js_divergence = jensenshannon(data_proportions[i], data_proportions[j])\n",
    "        js_divergence_results[(datasets[i], datasets[j])] = js_divergence\n",
    "\n",
    "# Display the results\n",
    "for pair, divergence in js_divergence_results.items():\n",
    "    print(\n",
    "        f\"Jensen-Shannon Divergence between {pair[0]} and {pair[1]}: {divergence:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_odds_ratios_and_significance(data_proportions, raw_data, labels):\n",
    "    \"\"\"\n",
    "    Calculate odds ratios and significance for each dataset and technique.\n",
    "\n",
    "    Parameters:\n",
    "    - data_proportions: pd.DataFrame, proportions of each technique in each dataset.\n",
    "    - raw_data: list of DataFrames, one for each dataset containing raw counts.\n",
    "    - labels: list, persuasion technique labels.\n",
    "\n",
    "    Returns:\n",
    "    - odds_ratios_df: pd.DataFrame of odds ratios.\n",
    "    - significance_matrix: pd.DataFrame of boolean values indicating statistical significance.\n",
    "    \"\"\"\n",
    "    # Step 1: Calculate Odds Ratios\n",
    "    if not all(data_proportions.sum(axis=1).round(10) == 1.0):\n",
    "        raise ValueError(\"Proportions in each row must sum to 1.\")\n",
    "\n",
    "    odds_ratios_matrix = pd.DataFrame(columns=data_proportions.columns, index=data_proportions.index)\n",
    "    for i, domain in enumerate(data_proportions.index):\n",
    "        # Extract the dataset to compare\n",
    "        dataset_proportions = data_proportions[data_proportions.index == domain].squeeze()\n",
    "\n",
    "        # Combine the other datasets by averaging proportions\n",
    "        combined_df = data_proportions[data_proportions.index != domain]\n",
    "        combined_proportions = combined_df.mean(axis=0)\n",
    "\n",
    "        # Calculate odds for each technique in both combined and single dataset\n",
    "        odds_combined = combined_proportions / ((1 - combined_proportions) + 1e-10)\n",
    "        odds_dataset = dataset_proportions / ((1 - dataset_proportions) + 1e-10)\n",
    "\n",
    "        odds_ratios = odds_dataset / odds_combined\n",
    "        odds_ratios_matrix.loc[domain] = odds_ratios\n",
    "\n",
    "    # Transpose to match expected structure\n",
    "    odds_ratios_df = odds_ratios_matrix[labels].T\n",
    "\n",
    "    # Step 2: Create Raw Counts Dictionary\n",
    "    raw_counts = {}\n",
    "    for i, (dataset, df) in enumerate(zip(data_proportions.index, raw_data)):\n",
    "        df_a = raw_data[i]\n",
    "        df_b = raw_data[:i] + raw_data[i + 1 :]\n",
    "        for technique in labels:\n",
    "            df_a_technique = df_a[df_a[\"persuasion_techniques\"].apply(lambda x: technique in x)]\n",
    "            df_a_not_technique = df_a[df_a[\"persuasion_techniques\"].apply(lambda x: technique not in x)]\n",
    "\n",
    "            df_b_technique = pd.concat(\n",
    "                [\n",
    "                    df[df[\"persuasion_techniques\"].apply(lambda x: technique in x)]\n",
    "                    for df in df_b\n",
    "                ]\n",
    "            )\n",
    "            df_b_not_technique = pd.concat(\n",
    "                [\n",
    "                    df[df[\"persuasion_techniques\"].apply(lambda x: technique not in x)]\n",
    "                    for df in df_b\n",
    "                ]\n",
    "            )\n",
    "\n",
    "            a = len(df_a_technique)\n",
    "            b = len(df_a_not_technique)\n",
    "            c = len(df_b_technique)\n",
    "            d = len(df_b_not_technique)\n",
    "            raw_counts[(technique, dataset)] = {\"a\": a, \"b\": b, \"c\": c, \"d\": d}\n",
    "\n",
    "    # Step 3: Calculate Statistical Significance\n",
    "    significance_matrix = pd.DataFrame(index=odds_ratios_df.index, columns=odds_ratios_df.columns, dtype=bool)\n",
    "\n",
    "    for technique in odds_ratios_df.index:\n",
    "        for dataset in odds_ratios_df.columns:\n",
    "            counts = raw_counts[(technique, dataset)]\n",
    "            a, b, c, d = counts['a'], counts['b'], counts['c'], counts['d']\n",
    "            \n",
    "            _, p_value = fisher_exact([[a, b], [c, d]])\n",
    "            \n",
    "            significance_matrix.loc[technique, dataset] = p_value < 0.05\n",
    "\n",
    "    odds_ratios_df = odds_ratios_df.astype(float)\n",
    "    odds_ratios_df = odds_ratios_df.round(2)\n",
    "    odds_ratios_df = odds_ratios_df.reindex(yticklabels)\n",
    "    significance_matrix = significance_matrix.reindex(yticklabels)\n",
    "    \n",
    "    return odds_ratios_df, significance_matrix\n",
    "\n",
    "data_proportions_df = pd.DataFrame(data_proportions, index=[\"Islamic Issues\", \"COVID-19\", \"Climate Change\", \"Russo-Ukrainian War\"], columns=SEMEVAL_LABELS)\n",
    "data_proportions_df= data_proportions_df[yticklabels]\n",
    "odds_ratios_df, significance_mask = calculate_odds_ratios_and_significance(data_proportions_df, [cidii_df, covid_df, climate_fever_df, euvsdisinfo_df], SEMEVAL_LABELS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def highlight_significant_values(odds_ratios_df, significance_matrix):\n",
    "    \"\"\"\n",
    "    Highlight significant values in the odds_ratios_df based on the significance_matrix.\n",
    "\n",
    "    Parameters:\n",
    "    - odds_ratios_df: pd.DataFrame containing odds ratios.\n",
    "    - significance_matrix: pd.DataFrame containing True/False values indicating significance.\n",
    "\n",
    "    Returns:\n",
    "    - Styled DataFrame with significant values highlighted.\n",
    "    \"\"\"\n",
    "    def highlight_cell(is_significant):\n",
    "        if is_significant:\n",
    "            return \"background-color: red; font-weight: bold;\"\n",
    "        return \"\"\n",
    "\n",
    "    # Apply the highlighting function to each cell\n",
    "    styled_df = odds_ratios_df.style.format(\n",
    "        precision=2\n",
    "    ).apply(\n",
    "        lambda row: [\n",
    "            highlight_cell(significance_matrix.loc[row.name, col])\n",
    "            for col in odds_ratios_df.columns\n",
    "        ],\n",
    "        axis=1\n",
    "    )\n",
    "\n",
    "    return styled_df\n",
    "\n",
    "odds_ratios_df.index = [idx.replace(\"_\", \" \") for idx in odds_ratios_df.index]\n",
    "significance_mask.index = [idx.replace(\"_\", \" \") for idx in significance_mask.index]\n",
    "styled_odds_ratios_df = highlight_significant_values(odds_ratios_df, significance_mask)\n",
    "styled_odds_ratios_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cidii_df = load_dataset(\"cidii\")\n",
    "cidii_liwc_df = pd.read_csv(\"datasets/liwc/liwc_cidii.csv\")\n",
    "cidii_liwc_df = cidii_liwc_df[\n",
    "    cidii_liwc_df[[\"id_sentence\", \"id_article\"]]\n",
    "    .apply(tuple, axis=1)\n",
    "    .isin(cidii_df[[\"id_sentence\", \"id_article\"]].apply(tuple, axis=1))\n",
    "].reset_index(drop=True)\n",
    "\n",
    "covid_df = load_dataset(\"covid\")\n",
    "covid_liwc_df = pd.read_csv(\"datasets/liwc/liwc_covid.csv\")\n",
    "covid_liwc_df = covid_liwc_df[\n",
    "    covid_liwc_df[[\"id_sentence\", \"id_article\"]]\n",
    "    .apply(tuple, axis=1)\n",
    "    .isin(covid_df[[\"id_sentence\", \"id_article\"]].apply(tuple, axis=1))\n",
    "].reset_index(drop=True)\n",
    "\n",
    "climate_fever_df = load_dataset(\"climate_fever\")\n",
    "climate_fever_liwc_df = pd.read_csv(\"datasets/liwc/liwc_climate_fever.csv\")\n",
    "climate_fever_liwc_df = climate_fever_liwc_df[\n",
    "    climate_fever_liwc_df[[\"id_sentence\", \"id_article\"]]\n",
    "    .apply(tuple, axis=1)\n",
    "    .isin(climate_fever_df[[\"id_sentence\", \"id_article\"]].apply(tuple, axis=1))\n",
    "].reset_index(drop=True)\n",
    "\n",
    "euvsdisinfo_df = load_dataset(\"euvsdisinfo\")\n",
    "euvsdisinfo_liwc_df = pd.read_csv(\"datasets/liwc/liwc_euvsdisinfo.csv\")\n",
    "euvsdisinfo_liwc_df = euvsdisinfo_liwc_df[\n",
    "    euvsdisinfo_liwc_df[[\"id_sentence\", \"id_article\"]]\n",
    "    .apply(tuple, axis=1)\n",
    "    .isin(euvsdisinfo_df[[\"id_sentence\", \"id_article\"]].apply(tuple, axis=1))\n",
    "].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_liwc_by_technique(df):\n",
    "    \"\"\"\n",
    "    Calculate the average LIWC scores for each persuasion technique in the dataset.\n",
    "\n",
    "    Parameters:\n",
    "    - df: DataFrame containing columns 'persuasion_techniques' and LIWC features.\n",
    "\n",
    "    Returns:\n",
    "    - A DataFrame with persuasion techniques as rows and average LIWC scores as columns.\n",
    "    \"\"\"\n",
    "\n",
    "    df[\"persuasion_techniques\"] = df[\"persuasion_techniques\"].fillna(\"\")\n",
    "\n",
    "    unique_techniques = SEMEVAL_LABELS\n",
    "\n",
    "    liwc_features = df.columns[df.columns.get_loc(\"Segment\") + 1 :]\n",
    "\n",
    "    average_liwc_scores = {}\n",
    "\n",
    "    for technique in unique_techniques:\n",
    "        technique_rows = df[\n",
    "            df[\"persuasion_techniques\"].str.contains(technique, regex=False)\n",
    "        ]\n",
    "\n",
    "        technique_mean = technique_rows[liwc_features].sum()\n",
    "\n",
    "        average_liwc_scores[technique] = technique_mean\n",
    "\n",
    "    average_liwc_df = pd.DataFrame(average_liwc_scores).T\n",
    "    average_liwc_df.index.name = \"Persuasion Technique\"\n",
    "\n",
    "    return average_liwc_df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_effect_size_heatmaps(\n",
    "    persuasion_technique: str, datasets: list, dataset_names: list, top_n: int\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes effect sizes for each LIWC feature in a given persuasion technique\n",
    "    and plots heatmaps for each dataset, displaying only the top_n LIWC features\n",
    "    with the highest effect sizes.\n",
    "\n",
    "    Parameters:\n",
    "        persuasion_technique (str): The persuasion technique for which to calculate effect sizes.\n",
    "        datasets (list): A list of 4 DataFrames, each representing a dataset with LIWC features as columns.\n",
    "                         Each DataFrame should contain rows corresponding to various persuasion techniques,\n",
    "                         including the one specified in persuasion_technique.\n",
    "        dataset_names (list): A list of 4 strings representing the names of each dataset, in the same order.\n",
    "        top_n (int): The number of top LIWC features to display based on highest effect size.\n",
    "    \"\"\"\n",
    "    assert (\n",
    "        len(datasets) == 4 and len(dataset_names) == 4\n",
    "    ), \"Please provide exactly 4 datasets and 4 dataset names.\"\n",
    "\n",
    "    liwc_features = datasets[0].columns\n",
    "    for dataset in datasets:\n",
    "        assert list(dataset.columns) == list(\n",
    "            liwc_features\n",
    "        ), \"All datasets must have the same LIWC feature columns.\"\n",
    "\n",
    "    technique_means = [dataset.loc[persuasion_technique] for dataset in datasets]\n",
    "\n",
    "    effect_size_dicts = []\n",
    "\n",
    "    for i, dataset_mean in enumerate(technique_means):\n",
    "        effect_sizes = []\n",
    "        comparison_names = []\n",
    "\n",
    "        for j, other_mean in enumerate(technique_means):\n",
    "            if i != j:\n",
    "                effect_size = dataset_mean / other_mean\n",
    "                effect_sizes.append(effect_size)\n",
    "                comparison_names.append(dataset_names[j])\n",
    "\n",
    "        effect_size_df = pd.DataFrame(\n",
    "            effect_sizes, index=comparison_names, columns=liwc_features\n",
    "        )\n",
    "        effect_size_dicts.append(effect_size_df)\n",
    "\n",
    "    for i, effect_df in enumerate(effect_size_dicts):\n",
    "        sorted_columns = (\n",
    "            effect_df.mean(axis=0).sort_values(ascending=False).index[:top_n]\n",
    "        )\n",
    "        effect_df_top_n = effect_df[sorted_columns]\n",
    "\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        sns.heatmap(effect_df_top_n, annot=True, cmap=\"coolwarm\", cbar=True)\n",
    "        plt.title(\n",
    "            f\"Top {top_n} Effect Size Heatmap for {dataset_names[i]} - Persuasion Technique: {persuasion_technique}\"\n",
    "        )\n",
    "        plt.xlabel(\"LIWC Features\")\n",
    "        plt.ylabel(\"Comparison Dataset\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_cidii_liwc = calculate_average_liwc_by_technique(cidii_liwc_df)\n",
    "avg_covid_liwc = calculate_average_liwc_by_technique(covid_liwc_df)\n",
    "avg_climate_fever_liwc = calculate_average_liwc_by_technique(climate_fever_liwc_df)\n",
    "avg_euvsdisinfo_liwc = calculate_average_liwc_by_technique(euvsdisinfo_liwc_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "liwc_categories = {\n",
    "    \"Summary Variables\": [\n",
    "        \"WC\",\n",
    "        \"Analytic\",\n",
    "        \"Clout\",\n",
    "        \"Authentic\",\n",
    "        \"Tone\",\n",
    "        \"WPS\",\n",
    "        \"BigWords\",\n",
    "        \"Dic\",\n",
    "    ],\n",
    "    \"Linguistic Variables\": [\n",
    "        \"function\",\n",
    "        \"pronoun\",\n",
    "        \"ppron\",\n",
    "        \"i\",\n",
    "        \"we\",\n",
    "        \"you\",\n",
    "        \"shehe\",\n",
    "        \"they\",\n",
    "        \"ipron\",\n",
    "        \"det\",\n",
    "        \"article\",\n",
    "        \"number\",\n",
    "        \"prep\",\n",
    "        \"auxverb\",\n",
    "        \"adverb\",\n",
    "        \"conj\",\n",
    "        \"negate\",\n",
    "        \"verb\",\n",
    "        \"adj\",\n",
    "        \"quantity\",\n",
    "    ],\n",
    "    \"Psychological Processes\": [\n",
    "        \"Drives\",\n",
    "        \"affiliation\",\n",
    "        \"achieve\",\n",
    "        \"power\",\n",
    "        # \"Cognition\",\n",
    "        \"allnone\",\n",
    "        \"cogproc\",\n",
    "        \"insight\",\n",
    "        \"cause\",\n",
    "        \"discrep\",\n",
    "        \"tentat\",\n",
    "        \"certitude\",\n",
    "        \"differ\",\n",
    "        \"memory\",\n",
    "        \"Affect\",\n",
    "        \"tone_pos\",\n",
    "        \"tone_neg\",\n",
    "        \"emotion\",\n",
    "        \"emo_pos\",\n",
    "        \"emo_neg\",\n",
    "        \"emo_anx\",\n",
    "        \"emo_anger\",\n",
    "        \"emo_sad\",\n",
    "        \"swear\",\n",
    "    ],\n",
    "    \"Social processes\": [\n",
    "        \"socbehav\",\n",
    "        \"prosocial\",\n",
    "        \"polite\",\n",
    "        \"conflict\",\n",
    "        \"moral\",\n",
    "        \"comm\",\n",
    "        \"socrefs\",\n",
    "        \"family\",\n",
    "        \"friend\",\n",
    "        \"female\",\n",
    "        \"male\",\n",
    "    ],\n",
    "    \"Expanded Dictionary\": [\"Culture\", \"politic\", \"ethnicity\", \"tech\"],\n",
    "    \"Lifestyle\": [\"leisure\", \"home\", \"work\", \"money\", \"relig\"],\n",
    "    \"Physical\": [\n",
    "        \"health\",\n",
    "        \"illness\",\n",
    "        \"wellness\",\n",
    "        \"mental\",\n",
    "        \"substances\",\n",
    "        \"sexual\",\n",
    "        \"food\",\n",
    "        \"death\",\n",
    "    ],\n",
    "    \"States\": [\"need\", \"want\", \"acquire\", \"lack\", \"fulfill\", \"fatigue\"],\n",
    "    \"Motives\": [\"reward\", \"risk\", \"curiosity\", \"allure\"],\n",
    "    \"Time orientation\": [\"time\", \"focuspast\", \"focuspresent\", \"focusfuture\"],\n",
    "    \"Conversational\": [\"netspeak\", \"assent\", \"nonflu\", \"filler\"],\n",
    "    # \"Affect\": [\n",
    "    #     \"tone_pos\",\n",
    "    #     \"tone_neg\",\n",
    "    #     \"emotion\",\n",
    "    #     \"emo_pos\",\n",
    "    #     \"emo_neg\",\n",
    "    #     \"emo_anx\",\n",
    "    #     \"emo_anger\",\n",
    "    #     \"emo_sad\",\n",
    "    # ],\n",
    "    # \"Cognition\": [\n",
    "    #     \"allnone\",\n",
    "    #     \"cogproc\",\n",
    "    #     \"insight\",\n",
    "    #     \"cause\",\n",
    "    #     \"discrep\",\n",
    "    #     \"tentat\",\n",
    "    #     \"certitude\",\n",
    "    #     \"differ\",\n",
    "    #     \"memory\",\n",
    "    # ],\n",
    "    \"Drives\": [\"affiliation\", \"achieve\", \"power\"],\n",
    "    \"Culture\": [\"politic\", \"ethnicity\", \"tech\"],\n",
    "    \"Perception\": [\n",
    "        \"attention\",\n",
    "        \"motion\",\n",
    "        \"space\",\n",
    "        \"visual\",\n",
    "        \"auditory\",\n",
    "        \"feeling\",\n",
    "        \"time\",\n",
    "        \"focuspast\",\n",
    "        \"focuspresent\",\n",
    "        \"focusfuture\",\n",
    "    ],\n",
    "    # \"Punctuation\": [\n",
    "    #     \"AllPunc\", \"Period\", \"Comma\", \"QMark\", \"Exclam\", \"Apostro\", \"OtherP\"\n",
    "    # ]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import patches as mpatches\n",
    "\n",
    "\n",
    "def plot_consolidated_top_features(\n",
    "    datasets, persuasion_technique, top_n, liwc_categories\n",
    "):\n",
    "    # Step 1: Extract top N features for each dataset and consolidate all unique features\n",
    "    all_top_features = set()\n",
    "    for dataset_name, avg_df in datasets.items():\n",
    "        heatmap_data = []\n",
    "        for category, features_list in liwc_categories.items():\n",
    "            for feature in features_list:\n",
    "                num = avg_df.loc[persuasion_technique, feature]\n",
    "                div = avg_df.loc[:, feature][\n",
    "                    np.nonzero(avg_df.loc[:, feature])[0]\n",
    "                ].mean()\n",
    "                value = num / div if div != 0 else 0\n",
    "                heatmap_data.append((feature, value))\n",
    "\n",
    "        # Select top N features for the current dataset\n",
    "        top_features = sorted(heatmap_data, key=lambda x: x[1], reverse=True)[:top_n]\n",
    "        all_top_features.update([feature for feature, _ in top_features])\n",
    "\n",
    "    # Step 2: Consolidate all unique features into a definitive list\n",
    "    definitive_features = list(all_top_features)\n",
    "\n",
    "    # Map features to their coarse groups\n",
    "    feature_group_map = {}\n",
    "    for group, features in liwc_categories.items():\n",
    "        for feature in features:\n",
    "            if feature in definitive_features:\n",
    "                feature_group_map[feature] = group\n",
    "\n",
    "    # Step 3: Populate values for all datasets for these definitive features\n",
    "    consolidated_data = {}\n",
    "    for feature in definitive_features:\n",
    "        consolidated_data[feature] = {}\n",
    "        for dataset_name, avg_df in datasets.items():\n",
    "            num = avg_df.loc[persuasion_technique, feature]\n",
    "            div = avg_df.loc[:, feature][np.nonzero(avg_df.loc[:, feature])[0]].mean()\n",
    "            value = num / div if div != 0 else 0\n",
    "            consolidated_data[feature][dataset_name] = value\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    consolidated_df = pd.DataFrame.from_dict(consolidated_data, orient=\"index\").fillna(\n",
    "        0\n",
    "    )\n",
    "\n",
    "    # Step 4: Sort and organize features by groups\n",
    "    sorted_features = sorted(\n",
    "        definitive_features, key=lambda f: (feature_group_map[f], f)\n",
    "    )\n",
    "    sorted_df = consolidated_df.loc[sorted_features]\n",
    "\n",
    "    # Step 5: Normalize each column individually to adjust color gradient per column\n",
    "    # sorted_df = sorted_df.apply(lambda x: (x - x.min()) / (x.max() - x.min()) if x.max() > x.min() else x, axis=0)\n",
    "\n",
    "    # Step 6: Plot the heatmap\n",
    "    plt.figure(figsize=(15, 4))  # Adjust height based on features\n",
    "    sns.heatmap(\n",
    "        sorted_df.transpose(),\n",
    "        annot=True,\n",
    "        cmap=\"YlGnBu\",\n",
    "        fmt=\".2f\",\n",
    "        cbar_kws={\"label\": \"Normalized Value (per column)\"},\n",
    "        linewidths=0.5,\n",
    "        xticklabels=sorted_features,\n",
    "    )\n",
    "    plt.xlabel(\"LIWC Feature\")\n",
    "    plt.ylabel(\"Disinformation Domain\")\n",
    "    plt.xticks(rotation=45, ha=\"right\")\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Add legend for groups\n",
    "    unique_groups = list({feature_group_map[f] for f in definitive_features})\n",
    "    group_patches = [mpatches.Patch(label=group) for group in unique_groups]\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage\n",
    "datasets = {\n",
    "    \"Islamic Issues\": avg_cidii_liwc,\n",
    "    \"COVID-19\": avg_covid_liwc,\n",
    "    \"Climate Change\": avg_climate_fever_liwc,\n",
    "    \"Russo-Ukrainian War\": avg_euvsdisinfo_liwc,\n",
    "}\n",
    "\n",
    "technique = \"Appeal_to_Fear-Prejudice\"\n",
    "plot_consolidated_top_features(datasets, technique, 8, liwc_categories)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cidii_liwc_df[\"Analytic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_semeval_labels(df):\n",
    "    \"\"\"\n",
    "    Encode SEMEVAL labels and normalize LIWC features.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): Input DataFrame containing LIWC features and persuasion techniques.\n",
    "\n",
    "    Returns:\n",
    "    - labels (np.array): Binary matrix for persuasion techniques.\n",
    "    - standardized_features (np.array): Normalized LIWC features.\n",
    "    - feature_names (list): List of LIWC feature names.\n",
    "    \"\"\"\n",
    "    labels = np.zeros((len(df), len(SEMEVAL_LABELS)))\n",
    "    for i, row in df.iterrows():\n",
    "        for label in [\n",
    "            persuasion\n",
    "            for persuasion in row[\"persuasion_techniques\"].split(\",\")\n",
    "            if persuasion in SEMEVAL_LABELS\n",
    "        ]:\n",
    "            labels[i, SEMEVAL_LABELS.index(label)] = 1\n",
    "\n",
    "    # Flatten and extract LIWC feature names\n",
    "    features = [feature for _, feature in liwc_categories.items()]\n",
    "    feature_names = [f for sublist in features for f in sublist]\n",
    "\n",
    "    # Extract LIWC features and standardize\n",
    "    liwc_features = df.loc[:, feature_names].to_numpy()\n",
    "    # scaler = StandardScaler()\n",
    "    # standardized_features = scaler.fit_transform(liwc_features)\n",
    "    standardized_features = liwc_features\n",
    "\n",
    "    return labels, standardized_features\n",
    "\n",
    "\n",
    "# Encode datasets with feature and technique names\n",
    "cidii_labels, cidii_features = encode_semeval_labels(cidii_liwc_df)\n",
    "covid_labels, covid_features = encode_semeval_labels(covid_liwc_df)\n",
    "climate_fever_labels, climate_fever_features = encode_semeval_labels(\n",
    "    climate_fever_liwc_df\n",
    ")\n",
    "euvsdisinfo_labels, euvsdisinfo_features = encode_semeval_labels(euvsdisinfo_liwc_df)\n",
    "\n",
    "liwc_feature_names = [f for sublist in liwc_categories.values() for f in sublist]\n",
    "\n",
    "\n",
    "# Compute correlations with actual feature and technique names\n",
    "def compute_correlations_with_significance(\n",
    "    features, labels, feature_names, technique_names\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute point-biserial correlations and p-values with named indices.\n",
    "\n",
    "    Parameters:\n",
    "    - features (np.array): Matrix of LIWC features (N x F).\n",
    "    - labels (np.array): Binary matrix of persuasion techniques (N x T).\n",
    "    - feature_names (list): List of LIWC feature names.\n",
    "    - technique_names (list): List of persuasion technique names.\n",
    "\n",
    "    Returns:\n",
    "    - correlation_matrix (pd.DataFrame): DataFrame of correlation coefficients.\n",
    "    - p_value_matrix (pd.DataFrame): DataFrame of p-values.\n",
    "    \"\"\"\n",
    "    features = np.nan_to_num(features, nan=0)\n",
    "    num_features = features.shape[1]\n",
    "    num_techniques = labels.shape[1]\n",
    "\n",
    "    # Use feature and technique names for indexing\n",
    "    correlation_matrix = pd.DataFrame(index=feature_names, columns=technique_names)\n",
    "    p_value_matrix = pd.DataFrame(index=feature_names, columns=technique_names)\n",
    "\n",
    "    # Compute correlations for each feature-technique pair\n",
    "    for feature_idx in range(num_features):\n",
    "        for technique_idx in range(num_techniques):\n",
    "            corr, p_val = pointbiserialr(\n",
    "                features[:, feature_idx], labels[:, technique_idx]\n",
    "            )\n",
    "            correlation_matrix.iloc[feature_idx, technique_idx] = corr\n",
    "            p_value_matrix.iloc[feature_idx, technique_idx] = p_val\n",
    "\n",
    "    return correlation_matrix, p_value_matrix\n",
    "\n",
    "\n",
    "# Define SEMEVAL_LABELS as the technique names\n",
    "technique_names = SEMEVAL_LABELS\n",
    "\n",
    "# Compute correlations for each dataset with named indices\n",
    "cidii_correlations, cidii_p_values = compute_correlations_with_significance(\n",
    "    cidii_features, cidii_labels, liwc_feature_names, technique_names\n",
    ")\n",
    "covid_correlations, covid_p_values = compute_correlations_with_significance(\n",
    "    covid_features, covid_labels, liwc_feature_names, technique_names\n",
    ")\n",
    "climate_fever_correlations, climate_fever_p_values = (\n",
    "    compute_correlations_with_significance(\n",
    "        climate_fever_features,\n",
    "        climate_fever_labels,\n",
    "        liwc_feature_names,\n",
    "        technique_names,\n",
    "    )\n",
    ")\n",
    "euvsdisinfo_correlations, euvsdisinfo_p_values = compute_correlations_with_significance(\n",
    "    euvsdisinfo_features, euvsdisinfo_labels, liwc_feature_names, technique_names\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_correlation_summary_with_absolute_and_pvalues(\n",
    "    correlation_matrix, p_value_matrix, dataset_name, p_value_threshold=0.05\n",
    "):\n",
    "    # Flatten the correlation matrix\n",
    "    flattened = correlation_matrix.stack().reset_index()\n",
    "    flattened.columns = [\"LIWC Feature\", \"Persuasion Technique\", \"Correlation\"]\n",
    "    flattened = flattened.dropna()\n",
    "\n",
    "    # Flatten the p-value matrix\n",
    "    p_values = p_value_matrix.stack().reset_index(drop=True)\n",
    "    flattened[\"p-value\"] = p_values\n",
    "\n",
    "    # Add the dataset name\n",
    "    flattened[\"Dataset\"] = dataset_name\n",
    "\n",
    "    # Add absolute correlation column\n",
    "    flattened[\"Absolute Correlation\"] = flattened[\"Correlation\"].abs()\n",
    "\n",
    "    # Add interpretation column\n",
    "    def interpret_correlation(row):\n",
    "        if row[\"p-value\"] < p_value_threshold:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    flattened[\"Significant\"] = flattened.apply(interpret_correlation, axis=1)\n",
    "    return flattened\n",
    "\n",
    "\n",
    "# Generate summary tables for each dataset with p-values and filter for significance\n",
    "p_value_threshold = 0.05\n",
    "\n",
    "cidii_summary = generate_correlation_summary_with_absolute_and_pvalues(\n",
    "    cidii_correlations, cidii_p_values, \"Islamic Issues\", p_value_threshold\n",
    ")\n",
    "covid_summary = generate_correlation_summary_with_absolute_and_pvalues(\n",
    "    covid_correlations, covid_p_values, \"COVID-19\", p_value_threshold\n",
    ")\n",
    "climate_fever_summary = generate_correlation_summary_with_absolute_and_pvalues(\n",
    "    climate_fever_correlations,\n",
    "    climate_fever_p_values,\n",
    "    \"Climate Change\",\n",
    "    p_value_threshold,\n",
    ")\n",
    "euvsdisinfo_summary = generate_correlation_summary_with_absolute_and_pvalues(\n",
    "    euvsdisinfo_correlations,\n",
    "    euvsdisinfo_p_values,\n",
    "    \"Russo-Ukrainian War\",\n",
    "    p_value_threshold,\n",
    ")\n",
    "\n",
    "# Get only significant correlations\n",
    "cidii_significant = cidii_summary[cidii_summary[\"Significant\"]]\n",
    "covid_significant = covid_summary[covid_summary[\"Significant\"]]\n",
    "climate_fever_significant = climate_fever_summary[climate_fever_summary[\"Significant\"]]\n",
    "euvsdisinfo_significant = euvsdisinfo_summary[euvsdisinfo_summary[\"Significant\"]]\n",
    "\n",
    "# Ensure 'Absolute Correlation' is numeric in each dataset summary\n",
    "cidii_significant[\"Absolute Correlation\"] = pd.to_numeric(\n",
    "    cidii_significant[\"Absolute Correlation\"], errors=\"coerce\"\n",
    ")\n",
    "covid_significant[\"Absolute Correlation\"] = pd.to_numeric(\n",
    "    covid_significant[\"Absolute Correlation\"], errors=\"coerce\"\n",
    ")\n",
    "climate_fever_significant[\"Absolute Correlation\"] = pd.to_numeric(\n",
    "    climate_fever_significant[\"Absolute Correlation\"], errors=\"coerce\"\n",
    ")\n",
    "euvsdisinfo_significant[\"Absolute Correlation\"] = pd.to_numeric(\n",
    "    euvsdisinfo_significant[\"Absolute Correlation\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Remove duplicate rows\n",
    "cidii_significant = cidii_significant.drop_duplicates(\n",
    "    [\"LIWC Feature\", \"Persuasion Technique\", \"Dataset\"]\n",
    ")\n",
    "covid_significant = covid_significant.drop_duplicates(\n",
    "    [\"LIWC Feature\", \"Persuasion Technique\", \"Dataset\"]\n",
    ")\n",
    "climate_fever_significant = climate_fever_significant.drop_duplicates(\n",
    "    [\"LIWC Feature\", \"Persuasion Technique\", \"Dataset\"]\n",
    ")\n",
    "euvsdisinfo_significant = euvsdisinfo_significant.drop_duplicates(\n",
    "    [\"LIWC Feature\", \"Persuasion Technique\", \"Dataset\"]\n",
    ")\n",
    "\n",
    "# Extract the top 10 absolute correlations for each dataset\n",
    "topn = 20\n",
    "cidii_significant = cidii_significant.nlargest(topn, \"Absolute Correlation\")\n",
    "covid_significant = covid_significant.nlargest(topn, \"Absolute Correlation\")\n",
    "climate_fever_significant = climate_fever_significant.nlargest(\n",
    "    topn, \"Absolute Correlation\"\n",
    ")\n",
    "euvsdisinfo_significant = euvsdisinfo_significant.nlargest(topn, \"Absolute Correlation\")\n",
    "\n",
    "# Get all unique pairs of LIWC features and Persuasion Techniques\n",
    "all_tuples = []\n",
    "for dataset in [\n",
    "    cidii_significant,\n",
    "    covid_significant,\n",
    "    climate_fever_significant,\n",
    "    euvsdisinfo_significant,\n",
    "]:\n",
    "    all_tuples.extend(\n",
    "        list(\n",
    "            [\n",
    "                tuple(row)\n",
    "                for row in dataset[[\"LIWC Feature\", \"Persuasion Technique\"]].values\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "all_tuples = list(set(all_tuples))\n",
    "\n",
    "# Retrieve these features from the summary datasets\n",
    "cidii_summary = cidii_summary[\n",
    "    cidii_summary.apply(\n",
    "        lambda row: (row[\"LIWC Feature\"], row[\"Persuasion Technique\"]) in all_tuples,\n",
    "        axis=1,\n",
    "    )\n",
    "]\n",
    "covid_summary = covid_summary[\n",
    "    covid_summary.apply(\n",
    "        lambda row: (row[\"LIWC Feature\"], row[\"Persuasion Technique\"]) in all_tuples,\n",
    "        axis=1,\n",
    "    )\n",
    "]\n",
    "climate_fever_summary = climate_fever_summary[\n",
    "    climate_fever_summary.apply(\n",
    "        lambda row: (row[\"LIWC Feature\"], row[\"Persuasion Technique\"]) in all_tuples,\n",
    "        axis=1,\n",
    "    )\n",
    "]\n",
    "euvsdisinfo_summary = euvsdisinfo_summary[\n",
    "    euvsdisinfo_summary.apply(\n",
    "        lambda row: (row[\"LIWC Feature\"], row[\"Persuasion Technique\"]) in all_tuples,\n",
    "        axis=1,\n",
    "    )\n",
    "]\n",
    "\n",
    "# Combine top correlations into a single DataFrame for display\n",
    "top_correlations_all = pd.concat(\n",
    "    [cidii_summary, covid_summary, climate_fever_summary, euvsdisinfo_summary]\n",
    ")\n",
    "\n",
    "# Sort and remove any remaining duplicates based on feature, technique, and dataset\n",
    "top_correlations_all = top_correlations_all.sort_values(\"LIWC Feature\", ascending=False)\n",
    "\n",
    "# Display the final table with filtered significant results\n",
    "top_correlations_all[\n",
    "    [\"LIWC Feature\", \"Persuasion Technique\", \"Dataset\", \"Correlation\", \"p-value\"]\n",
    "]\n",
    "\n",
    "# Pivot the data to create a table with datasets as columns\n",
    "pivot_table = top_correlations_all.pivot_table(\n",
    "    index=[\"LIWC Feature\", \"Persuasion Technique\"],\n",
    "    columns=[\"Dataset\"],\n",
    "    values=\"Correlation\",\n",
    ")\n",
    "\n",
    "pivot_table.index = pd.MultiIndex.from_tuples(\n",
    "    [(technique, feature) for technique, feature in pivot_table.index],\n",
    "    names=[\"Persuasion Technique\", \"LIWC Feature\"],\n",
    ")\n",
    "\n",
    "# Transpose the table so the techniques and features are aligned as row levels\n",
    "pivot_table = pivot_table.T\n",
    "\n",
    "\n",
    "# Modify the mask to retain all values but annotate non-significant ones distinctly\n",
    "mask = pd.isnull(pivot_table)\n",
    "\n",
    "pivot_table = top_correlations_all.pivot_table(\n",
    "    index=[\"LIWC Feature\", \"Persuasion Technique\"],\n",
    "    columns=[\"Dataset\"],\n",
    "    values=\"Correlation\",\n",
    ").dropna()\n",
    "\n",
    "pivot_table.index = pd.MultiIndex.from_tuples(\n",
    "    [(technique, feature) for technique, feature in pivot_table.index],\n",
    "    names=[\"Persuasion Technique\", \"LIWC Feature\"],\n",
    ")\n",
    "\n",
    "# Transpose the table so the techniques and features are aligned as row levels\n",
    "pivot_table = pivot_table.T\n",
    "\n",
    "\n",
    "# Modify the mask to retain all values but annotate non-significant ones distinctly\n",
    "mask = pd.isnull(pivot_table)\n",
    "\n",
    "\n",
    "# Custom annotation formatting function\n",
    "def custom_fmt_with_significance(x, p):\n",
    "    \"\"\"\n",
    "    Format the annotation based on significance.\n",
    "    Add an asterisk (*) for statistically significant values (p < 0.05).\n",
    "    Non-significant values will appear without parentheses but formatted with leading zeros removed.\n",
    "    \"\"\"\n",
    "    if pd.isnull(x):\n",
    "        return \"\"\n",
    "    if p < 0.05:\n",
    "        return (\n",
    "            f\"{x:.2f}\".lstrip(\"0\").replace(\"-0\", \"-\") + \"\\n*\"\n",
    "        )  # Significant values with asterisk\n",
    "    else:\n",
    "        return f\"{x:.2f}\".lstrip(\"0\").replace(\n",
    "            \"-0\", \"-\"\n",
    "        )  # Non-significant values without asterisk\n",
    "\n",
    "\n",
    "p_value_pivot = top_correlations_all.pivot_table(\n",
    "    index=[\"LIWC Feature\", \"Persuasion Technique\"],\n",
    "    columns=[\"Dataset\"],\n",
    "    values=\"p-value\",\n",
    ").T\n",
    "\n",
    "# Create annotation data for heatmap with significance formatting\n",
    "annotations = pivot_table.apply(\n",
    "    lambda col: col.combine(\n",
    "        p_value_pivot[col.name],  # Match corresponding p-values\n",
    "        custom_fmt_with_significance,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Increase figure width to reduce label overlap\n",
    "plt.figure(figsize=(30, 15))  # Adjust width for better alignment\n",
    "\n",
    "# Generate heatmap\n",
    "ax = sns.heatmap(\n",
    "    pivot_table.fillna(0),\n",
    "    annot=annotations,  # Use the formatted annotations with significance\n",
    "    fmt=\"\",  # Leave formatting to custom function\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    cbar_kws={\"label\": \"Correlation\"},\n",
    "    annot_kws={\"fontsize\": 16},  # Adjust annotation font size for readability\n",
    ")\n",
    "\n",
    "# Adjust the color bar title font size\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(\"Correlation\", fontsize=16)\n",
    "\n",
    "# Adjust the color bar tick labels font size\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "\n",
    "technique_mapping = {\n",
    "    \"Causal_Oversimplification\": \"Causal Oversimpl.\",\n",
    "    \"Questioning_the_Reputation\": \"Quest. Reputation\",\n",
    "    \"False_Dilemma-No_Choice\": \"False Dilemma\",\n",
    "    \"Exaggeration-Minimisation\": \"Exag./Minim.\",\n",
    "    \"Conversation_Killer\": \"Conv. Killer\",\n",
    "    \"Name_Calling-Labeling\": \"Labelling\",\n",
    "    \"Loaded_Language\": \"Loaded Lang.\",\n",
    "    \"Repetition\": \"Repetition\",\n",
    "    \"Appeal_to_Fear-Prejudice\": \"Appeal Fear\",\n",
    "    \"Flag_Waving\": \"Flag Waving\",\n",
    "    \"Doubt\": \"Doubt\",\n",
    "    \"Appeal_to_Authority\": \"Appeal Authority\",\n",
    "    \"Appeal_to_Values\": \"Appeal Values\",\n",
    "    \"Slogans\": \"Slogans\",\n",
    "}\n",
    "\n",
    "# Apply the technique mapping to shorten technique names\n",
    "short_techniques = [\n",
    "    technique_mapping.get(tup[1], tup[1]) for tup in pivot_table.columns\n",
    "]\n",
    "features = [tup[0] for tup in pivot_table.columns]\n",
    "\n",
    "# Apply the formatted labels with split techniques and features\n",
    "ax.set_xticklabels(\n",
    "    short_techniques, rotation=90, ha=\"center\", fontsize=16, color=\"black\"\n",
    ")\n",
    "for idx, label in enumerate(features):\n",
    "    ax.text(\n",
    "        idx + 0.5,\n",
    "        -1.0,\n",
    "        label,\n",
    "        ha=\"center\",\n",
    "        fontsize=16,\n",
    "        rotation=90,\n",
    "        color=\"black\",\n",
    "        transform=ax.get_xaxis_transform(),\n",
    "    )\n",
    "\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=16, color=\"black\")\n",
    "\n",
    "# Title and axis labels\n",
    "plt.xlabel(\"Persuasion Technique and LIWC Feature\", fontsize=18, labelpad=100)\n",
    "plt.ylabel(\"Disinformation Domain\", fontsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/correlation_heatmap.pdf\", bbox_inches=\"tight\", dpi=600)\n",
    "plt.show()\n",
    "\n",
    "print(\n",
    "    \"This version shows the highest absolute correlations between LIWC features and persuasion techniques for each dataset.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_correlation_summary_with_absolute_and_pvalues(\n",
    "    correlation_matrix, p_value_matrix, dataset_name, p_value_threshold=0.05\n",
    "):\n",
    "    # Flatten the correlation matrix\n",
    "    flattened = correlation_matrix.stack().reset_index()\n",
    "    flattened.columns = [\"LIWC Feature\", \"Persuasion Technique\", \"Correlation\"]\n",
    "    flattened = flattened.dropna()\n",
    "\n",
    "    # Flatten the p-value matrix\n",
    "    p_values = p_value_matrix.stack().reset_index(drop=True)\n",
    "    flattened[\"p-value\"] = p_values\n",
    "\n",
    "    # Add the dataset name\n",
    "    flattened[\"Dataset\"] = dataset_name\n",
    "\n",
    "    # Add absolute correlation column\n",
    "    flattened[\"Absolute Correlation\"] = flattened[\"Correlation\"].abs()\n",
    "\n",
    "    # Add interpretation column\n",
    "    def interpret_correlation(row):\n",
    "        if row[\"p-value\"] < p_value_threshold:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "\n",
    "    flattened[\"Significant\"] = flattened.apply(interpret_correlation, axis=1)\n",
    "    return flattened\n",
    "\n",
    "\n",
    "# Generate summary tables for each dataset with p-values and filter for significance\n",
    "p_value_threshold = 0.05\n",
    "\n",
    "cidii_summary = generate_correlation_summary_with_absolute_and_pvalues(\n",
    "    cidii_correlations, cidii_p_values, \"Islamic Issues\", p_value_threshold\n",
    ")\n",
    "covid_summary = generate_correlation_summary_with_absolute_and_pvalues(\n",
    "    covid_correlations, covid_p_values, \"COVID-19\", p_value_threshold\n",
    ")\n",
    "climate_fever_summary = generate_correlation_summary_with_absolute_and_pvalues(\n",
    "    climate_fever_correlations,\n",
    "    climate_fever_p_values,\n",
    "    \"Climate Change\",\n",
    "    p_value_threshold,\n",
    ")\n",
    "euvsdisinfo_summary = generate_correlation_summary_with_absolute_and_pvalues(\n",
    "    euvsdisinfo_correlations,\n",
    "    euvsdisinfo_p_values,\n",
    "    \"Russo-Ukrainian War\",\n",
    "    p_value_threshold,\n",
    ")\n",
    "\n",
    "# Get only significant correlations\n",
    "cidii_significant = cidii_summary[cidii_summary[\"Significant\"]]\n",
    "covid_significant = covid_summary[covid_summary[\"Significant\"]]\n",
    "climate_fever_significant = climate_fever_summary[climate_fever_summary[\"Significant\"]]\n",
    "euvsdisinfo_significant = euvsdisinfo_summary[euvsdisinfo_summary[\"Significant\"]]\n",
    "\n",
    "# Ensure 'Absolute Correlation' is numeric in each dataset summary\n",
    "cidii_significant[\"Absolute Correlation\"] = pd.to_numeric(\n",
    "    cidii_significant[\"Absolute Correlation\"], errors=\"coerce\"\n",
    ")\n",
    "covid_significant[\"Absolute Correlation\"] = pd.to_numeric(\n",
    "    covid_significant[\"Absolute Correlation\"], errors=\"coerce\"\n",
    ")\n",
    "climate_fever_significant[\"Absolute Correlation\"] = pd.to_numeric(\n",
    "    climate_fever_significant[\"Absolute Correlation\"], errors=\"coerce\"\n",
    ")\n",
    "euvsdisinfo_significant[\"Absolute Correlation\"] = pd.to_numeric(\n",
    "    euvsdisinfo_significant[\"Absolute Correlation\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Remove duplicate rows\n",
    "cidii_significant = cidii_significant.drop_duplicates(\n",
    "    [\"LIWC Feature\", \"Persuasion Technique\", \"Dataset\"]\n",
    ")\n",
    "covid_significant = covid_significant.drop_duplicates(\n",
    "    [\"LIWC Feature\", \"Persuasion Technique\", \"Dataset\"]\n",
    ")\n",
    "climate_fever_significant = climate_fever_significant.drop_duplicates(\n",
    "    [\"LIWC Feature\", \"Persuasion Technique\", \"Dataset\"]\n",
    ")\n",
    "euvsdisinfo_significant = euvsdisinfo_significant.drop_duplicates(\n",
    "    [\"LIWC Feature\", \"Persuasion Technique\", \"Dataset\"]\n",
    ")\n",
    "\n",
    "# Extract the top 10 absolute correlations for each dataset\n",
    "# topn = 20\n",
    "# cidii_significant = cidii_significant.nlargest(topn, 'Absolute Correlation')\n",
    "# covid_significant = covid_significant.nlargest(topn, 'Absolute Correlation')\n",
    "# climate_fever_significant = climate_fever_significant.nlargest(topn, 'Absolute Correlation')\n",
    "# euvsdisinfo_significant = euvsdisinfo_significant.nlargest(topn, 'Absolute Correlation')\n",
    "\n",
    "# Get all unique pairs of LIWC features and Persuasion Techniques\n",
    "all_tuples = []\n",
    "for dataset in [\n",
    "    cidii_significant,\n",
    "    covid_significant,\n",
    "    climate_fever_significant,\n",
    "    euvsdisinfo_significant,\n",
    "]:\n",
    "    all_tuples.extend(\n",
    "        list(\n",
    "            [\n",
    "                tuple(row)\n",
    "                for row in dataset[[\"LIWC Feature\", \"Persuasion Technique\"]].values\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "all_tuples = list(set(all_tuples))\n",
    "\n",
    "# Retrieve these features from the summary datasets\n",
    "cidii_summary = cidii_summary[\n",
    "    cidii_summary.apply(\n",
    "        lambda row: (row[\"LIWC Feature\"], row[\"Persuasion Technique\"]) in all_tuples,\n",
    "        axis=1,\n",
    "    )\n",
    "]\n",
    "covid_summary = covid_summary[\n",
    "    covid_summary.apply(\n",
    "        lambda row: (row[\"LIWC Feature\"], row[\"Persuasion Technique\"]) in all_tuples,\n",
    "        axis=1,\n",
    "    )\n",
    "]\n",
    "climate_fever_summary = climate_fever_summary[\n",
    "    climate_fever_summary.apply(\n",
    "        lambda row: (row[\"LIWC Feature\"], row[\"Persuasion Technique\"]) in all_tuples,\n",
    "        axis=1,\n",
    "    )\n",
    "]\n",
    "euvsdisinfo_summary = euvsdisinfo_summary[\n",
    "    euvsdisinfo_summary.apply(\n",
    "        lambda row: (row[\"LIWC Feature\"], row[\"Persuasion Technique\"]) in all_tuples,\n",
    "        axis=1,\n",
    "    )\n",
    "]\n",
    "\n",
    "# Combine top correlations into a single DataFrame for display\n",
    "top_correlations_all = pd.concat(\n",
    "    [cidii_summary, covid_summary, climate_fever_summary, euvsdisinfo_summary]\n",
    ")\n",
    "\n",
    "# Sort and remove any remaining duplicates based on feature, technique, and dataset\n",
    "top_correlations_all = top_correlations_all.sort_values(\"LIWC Feature\", ascending=False)\n",
    "\n",
    "# Display the final table with filtered significant results\n",
    "top_correlations_all[\n",
    "    [\"LIWC Feature\", \"Persuasion Technique\", \"Dataset\", \"Correlation\", \"p-value\"]\n",
    "]\n",
    "\n",
    "### HERE ###\n",
    "\n",
    "# Merge significant results from all datasets\n",
    "merged_significant = (\n",
    "    cidii_significant.merge(\n",
    "        covid_significant,\n",
    "        on=[\"LIWC Feature\", \"Persuasion Technique\"],\n",
    "        how=\"outer\",\n",
    "        suffixes=(\"_cidii\", \"_covid\"),\n",
    "    )\n",
    "    .merge(\n",
    "        climate_fever_significant,\n",
    "        on=[\"LIWC Feature\", \"Persuasion Technique\"],\n",
    "        how=\"outer\",\n",
    "        suffixes=(\"\", \"_climate\"),\n",
    "    )\n",
    "    .merge(\n",
    "        euvsdisinfo_significant,\n",
    "        on=[\"LIWC Feature\", \"Persuasion Technique\"],\n",
    "        how=\"outer\",\n",
    "        suffixes=(\"\", \"_euvsdisinfo\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# Ensure correlation coefficients are filled with 0 if missing\n",
    "merged_significant = merged_significant.fillna(0)\n",
    "\n",
    "# Extract correlation columns\n",
    "merged_significant[\"Correlation_cidii\"] = pd.to_numeric(\n",
    "    merged_significant[\"Absolute Correlation_cidii\"], errors=\"coerce\"\n",
    ")\n",
    "merged_significant[\"Correlation_covid\"] = pd.to_numeric(\n",
    "    merged_significant[\"Absolute Correlation_covid\"], errors=\"coerce\"\n",
    ")\n",
    "merged_significant[\"Correlation_climate\"] = pd.to_numeric(\n",
    "    merged_significant[\"Absolute Correlation\"], errors=\"coerce\"\n",
    ")\n",
    "merged_significant[\"Correlation_euvsdisinfo\"] = pd.to_numeric(\n",
    "    merged_significant[\"Absolute Correlation_euvsdisinfo\"], errors=\"coerce\"\n",
    ")\n",
    "\n",
    "# Compute variance across the datasets for each pair\n",
    "merged_significant[\"Variance\"] = merged_significant[\n",
    "    [\n",
    "        \"Correlation_cidii\",\n",
    "        \"Correlation_covid\",\n",
    "        \"Correlation_climate\",\n",
    "        \"Correlation_euvsdisinfo\",\n",
    "    ]\n",
    "].var(axis=1)\n",
    "\n",
    "# Compute standard deviation (optional, instead of variance)\n",
    "merged_significant[\"Standard Deviation\"] = merged_significant[\n",
    "    [\n",
    "        \"Correlation_cidii\",\n",
    "        \"Correlation_covid\",\n",
    "        \"Correlation_climate\",\n",
    "        \"Correlation_euvsdisinfo\",\n",
    "    ]\n",
    "].std(axis=1)\n",
    "\n",
    "# Define the number of top variance pairs to select\n",
    "top_n = 50  # Adjust this value as needed\n",
    "\n",
    "# Sort by variance in descending order\n",
    "merged_significant_sorted = merged_significant.sort_values(\n",
    "    by=\"Variance\", ascending=False\n",
    ")\n",
    "\n",
    "# Select the top_n rows with the highest variance\n",
    "top_discrepant_pairs = merged_significant_sorted.head(top_n)\n",
    "\n",
    "# Display relevant columns for interpretation\n",
    "top_discrepant_pairs_summary = top_discrepant_pairs[\n",
    "    [\n",
    "        \"LIWC Feature\",\n",
    "        \"Persuasion Technique\",\n",
    "        \"Correlation_cidii\",\n",
    "        \"Correlation_covid\",\n",
    "        \"Correlation_climate\",\n",
    "        \"Correlation_euvsdisinfo\",\n",
    "        \"Variance\",\n",
    "        \"Standard Deviation\",\n",
    "    ]\n",
    "]\n",
    "\n",
    "top_correlations_all = top_correlations_all[\n",
    "    top_correlations_all.apply(\n",
    "        lambda x: (x[\"LIWC Feature\"], x[\"Persuasion Technique\"])\n",
    "        in list(\n",
    "            tuple(a)\n",
    "            for a in top_discrepant_pairs[\n",
    "                [\"LIWC Feature\", \"Persuasion Technique\"]\n",
    "            ].values\n",
    "        ),\n",
    "        axis=1,\n",
    "    )\n",
    "]\n",
    "\n",
    "### HERE ###\n",
    "\n",
    "\n",
    "# Pivot the data to create a table with datasets as columns\n",
    "pivot_table = top_correlations_all.pivot_table(\n",
    "    index=[\"LIWC Feature\", \"Persuasion Technique\"],\n",
    "    columns=[\"Dataset\"],\n",
    "    values=\"Correlation\",\n",
    ")\n",
    "\n",
    "pivot_table.index = pd.MultiIndex.from_tuples(\n",
    "    [(technique, feature) for technique, feature in pivot_table.index],\n",
    "    names=[\"Persuasion Technique\", \"LIWC Feature\"],\n",
    ")\n",
    "\n",
    "# Transpose the table so the techniques and features are aligned as row levels\n",
    "pivot_table = pivot_table.T\n",
    "\n",
    "\n",
    "# Modify the mask to retain all values but annotate non-significant ones distinctly\n",
    "mask = pd.isnull(pivot_table)\n",
    "\n",
    "pivot_table = top_correlations_all.pivot_table(\n",
    "    index=[\"LIWC Feature\", \"Persuasion Technique\"],\n",
    "    columns=[\"Dataset\"],\n",
    "    values=\"Correlation\",\n",
    ").dropna()\n",
    "\n",
    "pivot_table.index = pd.MultiIndex.from_tuples(\n",
    "    [(technique, feature) for technique, feature in pivot_table.index],\n",
    "    names=[\"Persuasion Technique\", \"LIWC Feature\"],\n",
    ")\n",
    "\n",
    "# Transpose the table so the techniques and features are aligned as row levels\n",
    "pivot_table = pivot_table.T\n",
    "\n",
    "\n",
    "# Modify the mask to retain all values but annotate non-significant ones distinctly\n",
    "mask = pd.isnull(pivot_table)\n",
    "\n",
    "\n",
    "# Custom annotation formatting function\n",
    "def custom_fmt_with_significance(x, p):\n",
    "    \"\"\"\n",
    "    Format the annotation based on significance.\n",
    "    Add an asterisk (*) for statistically significant values (p < 0.05).\n",
    "    Non-significant values will appear without parentheses but formatted with leading zeros removed.\n",
    "    \"\"\"\n",
    "    if pd.isnull(x):\n",
    "        return \"\"\n",
    "    if p < 0.05:\n",
    "        return (\n",
    "            f\"{x:.2f}\".lstrip(\"0\").replace(\"-0\", \"-\") + \"\\n*\"\n",
    "        )  # Significant values with asterisk\n",
    "    else:\n",
    "        return f\"{x:.2f}\".lstrip(\"0\").replace(\n",
    "            \"-0\", \"-\"\n",
    "        )  # Non-significant values without asterisk\n",
    "\n",
    "\n",
    "p_value_pivot = top_correlations_all.pivot_table(\n",
    "    index=[\"LIWC Feature\", \"Persuasion Technique\"],\n",
    "    columns=[\"Dataset\"],\n",
    "    values=\"p-value\",\n",
    ").T\n",
    "\n",
    "# Create annotation data for heatmap with significance formatting\n",
    "annotations = pivot_table.apply(\n",
    "    lambda col: col.combine(\n",
    "        p_value_pivot[col.name],  # Match corresponding p-values\n",
    "        custom_fmt_with_significance,\n",
    "    )\n",
    ")\n",
    "\n",
    "# Increase figure width to reduce label overlap\n",
    "plt.figure(figsize=(30, 17))  # Adjust width for better alignment\n",
    "\n",
    "# Generate heatmap\n",
    "ax = sns.heatmap(\n",
    "    pivot_table.fillna(0),\n",
    "    annot=annotations,  # Use the formatted annotations with significance\n",
    "    fmt=\"\",  # Leave formatting to custom function\n",
    "    cmap=\"coolwarm\",\n",
    "    center=0,\n",
    "    cbar_kws={\"label\": \"Correlation\"},\n",
    "    annot_kws={\"fontsize\": 16},  # Adjust annotation font size for readability\n",
    ")\n",
    "\n",
    "# Adjust the color bar title font size\n",
    "cbar = ax.collections[0].colorbar\n",
    "cbar.set_label(\"Correlation\", fontsize=16)\n",
    "\n",
    "# Adjust the color bar tick labels font size\n",
    "cbar.ax.tick_params(labelsize=14)\n",
    "\n",
    "technique_mapping = {\n",
    "    \"Causal_Oversimplification\": \"Causal Oversimpl.\",\n",
    "    \"Questioning_the_Reputation\": \"Quest. Reputation\",\n",
    "    \"False_Dilemma-No_Choice\": \"False Dilemma\",\n",
    "    \"Exaggeration-Minimisation\": \"Exag./Minim.\",\n",
    "    \"Conversation_Killer\": \"Conv. Killer\",\n",
    "    \"Name_Calling-Labeling\": \"Labelling\",\n",
    "    \"Loaded_Language\": \"Loaded Lang.\",\n",
    "    \"Repetition\": \"Repetition\",\n",
    "    \"Appeal_to_Fear-Prejudice\": \"Appeal Fear\",\n",
    "    \"Flag_Waving\": \"Flag Waving\",\n",
    "    \"Doubt\": \"Doubt\",\n",
    "    \"Appeal_to_Authority\": \"Appeal Authority\",\n",
    "    \"Appeal_to_Values\": \"Appeal Values\",\n",
    "    \"Slogans\": \"Slogans\",\n",
    "}\n",
    "\n",
    "# Apply the technique mapping to shorten technique names\n",
    "short_techniques = [\n",
    "    technique_mapping.get(tup[1], tup[1]) for tup in pivot_table.columns\n",
    "]\n",
    "features = [tup[0] for tup in pivot_table.columns]\n",
    "\n",
    "# Apply the formatted labels with split techniques and features\n",
    "ax.set_xticklabels(\n",
    "    short_techniques, rotation=90, ha=\"center\", fontsize=16, color=\"black\"\n",
    ")\n",
    "for idx, label in enumerate(features):\n",
    "    ax.text(\n",
    "        idx + 0.5,\n",
    "        -1.0,\n",
    "        label,\n",
    "        ha=\"center\",\n",
    "        fontsize=16,\n",
    "        rotation=90,\n",
    "        color=\"black\",\n",
    "        transform=ax.get_xaxis_transform(),\n",
    "    )\n",
    "\n",
    "ax.set_yticklabels(ax.get_yticklabels(), rotation=0, fontsize=16, color=\"black\")\n",
    "\n",
    "# Title and axis labels\n",
    "plt.xlabel(\"Persuasion Technique and LIWC Feature\", fontsize=18, labelpad=120)\n",
    "plt.ylabel(\"Disinformation Domain\", fontsize=18)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"figures/correlation_heatmap.pdf\", bbox_inches=\"tight\", dpi=600)\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\n",
    "    \"This version shows highest variance in correlation coefficients across datasets.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_examples(df, persuasion_technique, liwc_feature, n=5):\n",
    "    \"\"\"\n",
    "    Get example sentences from the dataset that contain the given persuasion technique and LIWC feature.\n",
    "\n",
    "    Parameters:\n",
    "    - df (pd.DataFrame): DataFrame containing the dataset.\n",
    "    - persuasion_technique (str): The persuasion technique to search for.\n",
    "    - liwc_feature (str): The LIWC feature to search for.\n",
    "    - n (int): Number of examples to retrieve.\n",
    "\n",
    "    Returns:\n",
    "    - examples (pd.DataFrame): DataFrame containing example sentences.\n",
    "    \"\"\"\n",
    "    technique_rows = df[df[\"persuasion_techniques\"].str.contains(persuasion_technique)]\n",
    "    feature_rows = technique_rows[technique_rows[liwc_feature] > 0]\n",
    "    feature_rows = feature_rows.sort_values(by=liwc_feature, ascending=False)\n",
    "    return feature_rows.head(n)[[\"Text\", liwc_feature]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# High use of \"cogproc\" words can indicate that the text involves reasoning, complex thinking, or explanation.\n",
    "get_examples(climate_fever_liwc_df, \"Appeal_to_Authority\", \"cogproc\", n=20).apply(\n",
    "    lambda x: print(x[\"Text\"]), axis=1\n",
    ");\n",
    "\n",
    "# NASA satellite data from the years 2000 through 2011 show the Earth's atmosphere is allowing far more heat to be released into space than alarmist computer models have predicted, reports a new study in the peer-reviewed science journal Remote Sensing\n",
    "# More than half of the 44 studies selected for publication found that raised levels of CO2 had little or no impact on marine life, including crabs, limpets, sea urchins and sponges\n",
    "# Some scientists believe that solar activity is more likely to influence today’s climate than carbon dioxide, and Dr Soon has compiled data showing temperature in America, Canada and Mexico rises and falls in line with solar activity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identifies words that express agreement or affirmation. This category includes terms such as \"agree,\" \"OK,\" and \"yes.\" The presence of assent words in a text can indicate a speaker's or writer's concurrence or acceptance.\n",
    "# Conversation killer statements that effectively end or stifle further discussion.\n",
    "\n",
    "df = get_examples(cidii_liwc_df, \"Conversation_Killer\", \"assent\", n=20)\n",
    "df.apply(lambda x: print(x[\"Text\"]), axis=1)\n",
    "df\n",
    "\n",
    "# There is absolutely no way to verify anything.\n",
    "# Absolutely nothing!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_examples(climate_fever_liwc_df, \"Appeal_to_Fear-Prejudice\", \"conflict\", n=20)\n",
    "df.apply(lambda x: print(x[\"Text\"]), axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_examples(climate_fever_liwc_df, \"Appeal_to_Fear-Prejudice\", \"death\", n=10)\n",
    "df.apply(lambda x: print(x[\"Text\"]), axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_examples(climate_fever_liwc_df, \"Appeal_to_Values\", \"emo_pos\", n=10)\n",
    "df.apply(lambda x: print(x[\"Text\"]), axis=1)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "persuasion_multi_domain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
